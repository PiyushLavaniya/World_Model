# Graph JEPA: Visual Architecture Guide

A visual, diagram-rich guide to understanding how the Graph JEPA model works.

---

## ðŸŽ¯ The Big Picture: What Are We Solving?

### The Challenge

**Given:** A knowledge graph that changes over time
```
                  TIME
    t=0          t=1          t=2          t=3
     â”‚            â”‚            â”‚            â”‚
     â–¼            â–¼            â–¼            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Graph   â”‚  â”‚ Graph   â”‚  â”‚ Graph   â”‚  â”‚ Graph   â”‚
â”‚  @ t=0  â”‚  â”‚  @ t=1  â”‚  â”‚  @ t=2  â”‚  â”‚  @ t=3  â”‚
â”‚         â”‚  â”‚         â”‚  â”‚         â”‚  â”‚         â”‚
â”‚  A â”€ B  â”‚  â”‚  A â”€ B  â”‚  â”‚  A â”€ B  â”‚  â”‚  A   B  â”‚
â”‚  â”‚   â”‚  â”‚  â”‚  â”‚   â”‚  â”‚  â”‚  â”‚   â”‚  â”‚  â”‚      â”‚  â”‚
â”‚  C â”€ D  â”‚  â”‚  C â”€ D  â”‚  â”‚  C   D  â”‚  â”‚  C â”€ D  â”‚
â”‚      â”‚  â”‚  â”‚  â”‚   â”‚  â”‚  â”‚  â”‚   â”‚  â”‚  â”‚  â”‚      â”‚
â”‚      E  â”‚  â”‚  E â”€ F  â”‚  â”‚  E â”€ F  â”‚  â”‚  E â”€ F  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Question:** Can we learn to predict future states from current context?

### Our Solution: Graph JEPA

**Learn to predict target graph embeddings from context graph embeddings**

```
Context Graph              Target Graph
(Partial, Corrupted)   â†’  (Complete, Clean)
      â†“                         â†“
   Encoder                   Encoder
      â†“                         â†“
   Embedding  â”€â”€PREDICTâ”€â”€â†’  Embedding
   [384-dim]                [384-dim]
```

---

## ðŸ“Š End-to-End Data Flow

### Step 1: Data Extraction (Neo4j â†’ JSONL)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Neo4j Knowledge Graph                â”‚
â”‚                                              â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”         â”‚
â”‚    â”‚  A  â”‚â”€â”€â”€â”€â–¶â”‚  B  â”‚â—€â”€â”€â”€â”€â”‚  C  â”‚         â”‚
â”‚    â””â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”˜         â”‚
â”‚       â”‚           â”‚           â”‚             â”‚
â”‚       â–¼           â–¼           â–¼             â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”         â”‚
â”‚    â”‚  D  â”‚â”€â”€â”€â”€â–¶â”‚  E  â”‚â—€â”€â”€â”€â”€â”‚  F  â”‚         â”‚
â”‚    â””â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
         [ Export Script: main.py ]
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         JSONL Files                          â”‚
â”‚                                              â”‚
â”‚  nodes.jsonl:                               â”‚
â”‚    {"uuid": "A", "label": "Entity", ...}    â”‚
â”‚    {"uuid": "B", "label": "Episodic", ...}  â”‚
â”‚                                              â”‚
â”‚  edges.jsonl:                               â”‚
â”‚    {"src": "A", "dst": "B", "type": ...}    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Step 2: Dataset Creation (JSONL â†’ Training Samples)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Dataset Creator: graph_jepa_dataset_creation â”‚
â”‚                                              â”‚
â”‚  FOR each temporal snapshot:                 â”‚
â”‚    FOR each anchor node:                     â”‚
â”‚      1. Extract k-hop subgraph (k=2)        â”‚
â”‚      2. Split: 75% context, 25% target      â”‚
â”‚      3. Corrupt context (drop edges, mask)   â”‚
â”‚      4. Save sample                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         dataset.jsonl (9,462 samples)        â”‚
â”‚                                              â”‚
â”‚  Sample:                                     â”‚
â”‚  {                                           â”‚
â”‚    "sample_id": 42,                         â”‚
â”‚    "t": "2025-06-15T12:00:00Z",            â”‚
â”‚    "anchor": {"uuid": "A", ...},           â”‚
â”‚    "context": {                             â”‚
â”‚      "nodes": [...],  # Corrupted           â”‚
â”‚      "edges": [...]   # 10% dropped         â”‚
â”‚    },                                        â”‚
â”‚    "target": {                              â”‚
â”‚      "nodes": [...],  # Clean               â”‚
â”‚      "edges": [...]   # Complete            â”‚
â”‚    }                                         â”‚
â”‚  }                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Step 3: Training (Dataset â†’ Model)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Training Loop                         â”‚
â”‚                                                          â”‚
â”‚  1. Load batch of (context, target) pairs              â”‚
â”‚  2. Encode both with GNN                                â”‚
â”‚  3. Predict target embedding from context               â”‚
â”‚  4. Compute loss (cosine similarity)                    â”‚
â”‚  5. Update student & predictor                          â”‚
â”‚  6. Update teacher via EMA                              â”‚
â”‚                                                          â”‚
â”‚  Repeat 40,000 times â†’  Trained Model                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ—ï¸ Model Architecture Deep Dive

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        GRAPH JEPA V0                          â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   CONTEXT PATH          â”‚  â”‚    TARGET PATH          â”‚   â”‚
â”‚  â”‚   (Trainable)           â”‚  â”‚    (EMA, Frozen)        â”‚   â”‚
â”‚  â”‚                         â”‚  â”‚                         â”‚   â”‚
â”‚  â”‚  Context Graph          â”‚  â”‚  Target Graph           â”‚   â”‚
â”‚  â”‚      (G_c)              â”‚  â”‚      (G_t)              â”‚   â”‚
â”‚  â”‚        â†“                â”‚  â”‚        â†“                â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚   â”‚
â”‚  â”‚  â”‚   Student   â”‚        â”‚  â”‚  â”‚   Teacher   â”‚        â”‚   â”‚
â”‚  â”‚  â”‚   Encoder   â”‚        â”‚  â”‚  â”‚   Encoder   â”‚        â”‚   â”‚
â”‚  â”‚  â”‚   (GNN)     â”‚        â”‚  â”‚  â”‚   (GNN)     â”‚        â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚   â”‚
â”‚  â”‚        â†“                â”‚  â”‚        â†“                â”‚   â”‚
â”‚  â”‚  Node Embeddings        â”‚  â”‚  Node Embeddings        â”‚   â”‚
â”‚  â”‚   [N_c, 384]            â”‚  â”‚   [N_t, 384]            â”‚   â”‚
â”‚  â”‚        â†“                â”‚  â”‚        â†“                â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚   â”‚
â”‚  â”‚  â”‚ Transformer â”‚        â”‚  â”‚  â”‚ Global Mean â”‚        â”‚   â”‚
â”‚  â”‚  â”‚ Predictor   â”‚        â”‚  â”‚  â”‚    Pool     â”‚        â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚   â”‚
â”‚  â”‚        â†“                â”‚  â”‚        â†“                â”‚   â”‚
â”‚  â”‚  Pred Embedding         â”‚  â”‚  Target Embedding       â”‚   â”‚
â”‚  â”‚    [B, 384]             â”‚  â”‚    [B, 384]             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚              â†“                          â†“                    â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ LOSS â†â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                     (Cosine Distance)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Input: Graph Representation

```
Graph G:
  
  Nodes: [
    {
      uuid: "entity_123",
      label: "Entity",
      attrs: {
        name: "TechCorp",
        summary: "A technology company",
        created_at: "2024-01-01T00:00:00Z",
        valid_at: "2024-01-01T00:00:00Z"
      }
    },
    ...
  ]
  
  Edges: [
    {
      src: "entity_123",
      dst: "entity_456",
      etype: "RELATES_TO",
      name: "PROVIDES_SERVICE_TO",
      fact: "TechCorp provides cloud services to RetailCo"
    },
    ...
  ]

â†“ CONVERT TO PyG Data â†“

PyG Data:
  x: [N, 259]        # Node features (text + time + label)
  edge_index: [2, E] # COO format edges
  edge_rel: [E]      # Relation type IDs
  batch: [N]         # Batch assignment
```

### Feature Engineering Pipeline

```
FOR each node:

  1. TEXT FEATURES (256-dim)
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ text = name + summary + content      â”‚
     â”‚   â†“                                  â”‚
     â”‚ char_ngrams = ["tec", "ech", "chc"...] â”‚
     â”‚   â†“                                  â”‚
     â”‚ hash each ngram â†’ bucket (0-255)     â”‚
     â”‚   â†“                                  â”‚
     â”‚ count vector [256]                   â”‚
     â”‚   â†“                                  â”‚
     â”‚ L2 normalize                         â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  2. TEMPORAL FEATURES (3-dim)
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ age = (t_now - valid_at) / 30 days   â”‚
     â”‚ tti = (invalid_at - t_now) / 30 days â”‚
     â”‚ is_open = 1 if still valid else 0    â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  3. LABEL EMBEDDING (32-dim)
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ label_id = {                         â”‚
     â”‚   "Episodic": 0,                     â”‚
     â”‚   "Entity": 1,                       â”‚
     â”‚   "Community": 2,                    â”‚
     â”‚   "Unknown": 3                       â”‚
     â”‚ }[node.label]                        â”‚
     â”‚   â†“                                  â”‚
     â”‚ embedding = LookupTable[label_id]    â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  CONCAT: [text_256 | temporal_3 | label_32] = 291-dim
  PROJECT: Linear(291 â†’ 384)
```

### Student/Teacher Encoder (GraphSAGE + Relations)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               GraphEncoder                          â”‚
â”‚                                                     â”‚
â”‚  Input: x [N, 259], edge_index [2, E], edge_rel [E] â”‚
â”‚                                                     â”‚
â”‚  1. Label Embedding                                â”‚
â”‚     label_emb = Embedding(label_id) â†’ [N, 32]      â”‚
â”‚     x_in = concat([x, label_emb]) â†’ [N, 291]       â”‚
â”‚                                                     â”‚
â”‚  2. Input Projection                               â”‚
â”‚     h = Linear(291 â†’ 384) â†’ [N, 384]               â”‚
â”‚     h = ReLU(h)                                    â”‚
â”‚     h = Dropout(h, p=0.1)                          â”‚
â”‚                                                     â”‚
â”‚  3. RelSAGEConv Layer 1                            â”‚
â”‚     h = RelSAGEConv(h, edge_index, edge_rel)       â”‚
â”‚     h = Dropout(h, p=0.1)                          â”‚
â”‚                                                     â”‚
â”‚  4. RelSAGEConv Layer 2                            â”‚
â”‚     h = RelSAGEConv(h, edge_index, edge_rel)       â”‚
â”‚     h = Dropout(h, p=0.1)                          â”‚
â”‚                                                     â”‚
â”‚  Output: h [N, 384]  (node embeddings)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### RelSAGEConv: Custom Message Passing

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            RelSAGEConv Layer                              â”‚
â”‚                                                           â”‚
â”‚  Input: x [N, 384], edge_index [2, E], edge_rel [E]      â”‚
â”‚                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  MESSAGE PASSING                                â”‚     â”‚
â”‚  â”‚                                                 â”‚     â”‚
â”‚  â”‚  For each edge (u â†’ v):                         â”‚     â”‚
â”‚  â”‚                                                 â”‚     â”‚
â”‚  â”‚    1. Get source features: h_u [384]           â”‚     â”‚
â”‚  â”‚    2. Get relation embedding: r [32]           â”‚     â”‚
â”‚  â”‚       r = Embedding(edge_rel[e])               â”‚     â”‚
â”‚  â”‚                                                 â”‚     â”‚
â”‚  â”‚    3. Compute message:                          â”‚     â”‚
â”‚  â”‚       m = W_neigh @ h_u + W_rel @ r            â”‚     â”‚
â”‚  â”‚       (transforms: [384]â†’[384] and [32]â†’[384]) â”‚     â”‚
â”‚  â”‚                                                 â”‚     â”‚
â”‚  â”‚    4. Aggregate messages for node v:           â”‚     â”‚
â”‚  â”‚       agg_v = mean({m_u for all u â†’ v})        â”‚     â”‚
â”‚  â”‚                                                 â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  UPDATE                                         â”‚     â”‚
â”‚  â”‚                                                 â”‚     â”‚
â”‚  â”‚  For each node v:                               â”‚     â”‚
â”‚  â”‚    h_v_new = W_self @ h_v + agg_v              â”‚     â”‚
â”‚  â”‚    h_v_new = LayerNorm(ReLU(h_v_new))          â”‚     â”‚
â”‚  â”‚                                                 â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                           â”‚
â”‚  Output: h_new [N, 384]                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Innovation:** Relation embeddings injected into messages, allowing the model to distinguish between different edge types.

### Predictor: Transformer Encoder

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Transformer Predictor                        â”‚
â”‚                                                          â”‚
â”‚  Input: node_h [N, 384], batch [N]                       â”‚
â”‚                                                          â”‚
â”‚  1. Convert to Dense Batch                              â”‚
â”‚     dense, mask = to_dense_batch(node_h, batch)         â”‚
â”‚     dense: [B, max_nodes, 384]                          â”‚
â”‚     mask: [B, max_nodes]  (True for real nodes)         â”‚
â”‚                                                          â”‚
â”‚  2. Prepend CLS Token                                   â”‚
â”‚     cls = LearnableParameter([1, 1, 384])               â”‚
â”‚     cls_expanded = cls.expand(B, 1, 384)                â”‚
â”‚     x = concat([cls_expanded, dense], dim=1)            â”‚
â”‚     x: [B, 1+max_nodes, 384]                            â”‚
â”‚                                                          â”‚
â”‚     mask_with_cls = concat([ones(B,1), mask], dim=1)    â”‚
â”‚                                                          â”‚
â”‚  3. Transformer Encoder (4 layers)                      â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚     â”‚  FOR each layer:                     â”‚            â”‚
â”‚     â”‚    1. LayerNorm                      â”‚            â”‚
â”‚     â”‚    2. Multi-Head Attention (6 heads) â”‚            â”‚
â”‚     â”‚       (with padding mask)             â”‚            â”‚
â”‚     â”‚    3. Residual connection             â”‚            â”‚
â”‚     â”‚    4. LayerNorm                      â”‚            â”‚
â”‚     â”‚    5. FFN (384 â†’ 1536 â†’ 384)        â”‚            â”‚
â”‚     â”‚    6. Residual connection             â”‚            â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                          â”‚
â”‚  4. Extract CLS Embedding                               â”‚
â”‚     y_cls = x[:, 0, :]  [B, 384]                        â”‚
â”‚                                                          â”‚
â”‚  5. Output Projection                                   â”‚
â”‚     out = Linear(384 â†’ 384)(y_cls)                      â”‚
â”‚                                                          â”‚
â”‚  6. L2 Normalize                                        â”‚
â”‚     out = out / ||out||_2                               â”‚
â”‚                                                          â”‚
â”‚  Output: pred_emb [B, 384]                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why Transformer?**
- Self-attention captures global graph context
- CLS token aggregates information from all nodes
- Handles variable-size graphs via padding masks

### Target Encoding (Teacher Path)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Teacher Encoding (EMA)                       â”‚
â”‚                                                          â”‚
â”‚  Input: target_graph (G_t)                               â”‚
â”‚                                                          â”‚
â”‚  1. Encode with Teacher GNN                             â”‚
â”‚     with torch.no_grad():  # Stop gradient!             â”‚
â”‚       h_t = TeacherEncoder(G_t)                         â”‚
â”‚       h_t: [N_t, 384]                                   â”‚
â”‚                                                          â”‚
â”‚  2. Global Mean Pooling                                 â”‚
â”‚     emb_target = global_mean_pool(h_t, batch)           â”‚
â”‚     emb_target: [B, 384]                                â”‚
â”‚                                                          â”‚
â”‚  3. L2 Normalize                                        â”‚
â”‚     emb_target = emb_target / ||emb_target||_2          â”‚
â”‚                                                          â”‚
â”‚  Output: target_emb [B, 384]                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Note: Teacher parameters NOT updated by backprop.
      Updated via EMA from student:
      
      Î¸_teacher â† 0.996 Ã— Î¸_teacher + 0.004 Ã— Î¸_student
```

---

## ðŸŽ“ Training Process Visualization

### Forward Pass

```
BATCH:
  Context Graphs (G_c): 16 graphs, various sizes
  Target Graphs (G_t): 16 graphs, various sizes

STEP 1: Student Encoding
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  G_c â†’ Student Encoder â†’ h_c       â”‚
  â”‚        [N_c, 384]                  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 2: Prediction
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  h_c â†’ Transformer Predictor       â”‚
  â”‚     â†’ pred_emb [16, 384]           â”‚
  â”‚     â†’ L2 normalize                 â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 3: Teacher Encoding (no grad)
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  G_t â†’ Teacher Encoder â†’ h_t       â”‚
  â”‚        [N_t, 384]                  â”‚
  â”‚     â†’ Global Mean Pool             â”‚
  â”‚     â†’ target_emb [16, 384]         â”‚
  â”‚     â†’ L2 normalize                 â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 4: Loss Computation
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Cosine Loss:                      â”‚
  â”‚    cos_sim = (pred âŠ™ target).sum() â”‚
  â”‚    loss = 1 - cos_sim              â”‚
  â”‚                                    â”‚
  â”‚  Optional InfoNCE:                 â”‚
  â”‚    logits = (pred @ target.T) / Ï„  â”‚
  â”‚    loss += CrossEntropy(logits, I) â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 5: Backward Pass
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  loss.backward()                   â”‚
  â”‚    â†’ Gradients for student         â”‚
  â”‚    â†’ Gradients for predictor       â”‚
  â”‚    â†’ NO gradients for teacher      â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 6: Optimizer Step
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Clip gradients (max_norm=1.0)     â”‚
  â”‚  optimizer.step()                  â”‚
  â”‚    â†’ Update student parameters     â”‚
  â”‚    â†’ Update predictor parameters   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STEP 7: EMA Update (Teacher)
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  FOR each parameter:               â”‚
  â”‚    Î¸_t â† m Ã— Î¸_t + (1-m) Ã— Î¸_s     â”‚
  â”‚    (m = 0.996)                     â”‚
  â”‚                                    â”‚
  â”‚  Teacher slowly follows student    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Learning Dynamics Over Time

```
Training Step:    0      5k     10k    15k    20k    25k    30k    35k    40k
                  â”‚       â”‚      â”‚      â”‚      â”‚      â”‚      â”‚      â”‚      â”‚
Loss:          1.2 â”€â”€â”€â”€â”€â†’ 1.0 â”€â†’ 0.9 â”€â†’ 0.88 â”€â†’ 0.87 â”€â†’ 0.87 â”€â†’ 0.87 â”€â†’ 0.87
                  â”‚       â”‚      â”‚      â”‚      â”‚      â”‚      â”‚      â”‚      â”‚
Cosine Sim:   0.60 â”€â”€â”€â”€â”€â†’ 0.72 â”€â†’ 0.78 â”€â†’ 0.82 â”€â†’ 0.84 â”€â†’ 0.85 â”€â†’ 0.86 â”€â†’ 0.86
                  â”‚       â”‚      â”‚      â”‚      â”‚      â”‚      â”‚      â”‚      â”‚
Top-1 Acc:    0.40 â”€â”€â”€â”€â”€â†’ 0.55 â”€â†’ 0.65 â”€â†’ 0.70 â”€â†’ 0.72 â”€â†’ 0.73 â”€â†’ 0.74 â”€â†’ 0.75
                  â”‚       â”‚      â”‚      â”‚      â”‚      â”‚      â”‚      â”‚      â”‚
Status:      [Fast Learning] [Steady Progress] [Fine-Tuning] [Convergence]
```

---

## ðŸ” Inference & Evaluation

### Inference Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INFERENCE PROCESS                          â”‚
â”‚                                                              â”‚
â”‚  1. Load Checkpoint                                          â”‚
â”‚     â”œâ”€ Student weights                                       â”‚
â”‚     â”œâ”€ Teacher weights                                       â”‚
â”‚     â””â”€ Predictor weights                                     â”‚
â”‚                                                              â”‚
â”‚  2. Set to Eval Mode                                         â”‚
â”‚     model.eval()                                             â”‚
â”‚     torch.no_grad()                                          â”‚
â”‚                                                              â”‚
â”‚  3. Load Test Dataset                                        â”‚
â”‚     dataset = JEPADatasetJSONL(test_path)                    â”‚
â”‚                                                              â”‚
â”‚  4. For Each Batch:                                          â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚     â”‚  ctx, tgt = next(dataloader)           â”‚              â”‚
â”‚     â”‚  pred, target = model(ctx, tgt)        â”‚              â”‚
â”‚     â”‚  metrics.update(pred, target)          â”‚              â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                                              â”‚
â”‚  5. Compute Aggregate Metrics                                â”‚
â”‚     â”œâ”€ Mean/std loss                                         â”‚
â”‚     â”œâ”€ Mean/std cosine similarity                            â”‚
â”‚     â”œâ”€ Top-1/Top-5 retrieval accuracy                        â”‚
â”‚     â””â”€ Embedding statistics                                  â”‚
â”‚                                                              â”‚
â”‚  6. Analyze Embeddings                                       â”‚
â”‚     â”œâ”€ PCA projection                                        â”‚
â”‚     â”œâ”€ K-means clustering                                    â”‚
â”‚     â”œâ”€ Similarity distributions                              â”‚
â”‚     â””â”€ Nearest neighbor analysis                             â”‚
â”‚                                                              â”‚
â”‚  7. Save Results                                             â”‚
â”‚     â”œâ”€ inference_results.json                                â”‚
â”‚     â”œâ”€ embeddings.pt                                         â”‚
â”‚     â””â”€ visualizations/*.png                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Evaluation Metrics Explained

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   EVALUATION METRICS                          â”‚
â”‚                                                               â”‚
â”‚  1. COSINE SIMILARITY                                         â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚     â”‚  cos(pred, target) = predÂ·target        â”‚              â”‚
â”‚     â”‚                      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€      â”‚              â”‚
â”‚     â”‚                      ||pred|| ||target|| â”‚              â”‚
â”‚     â”‚                                          â”‚              â”‚
â”‚     â”‚  Range: [-1, 1]                         â”‚              â”‚
â”‚     â”‚    1.0 = Perfect alignment               â”‚              â”‚
â”‚     â”‚    0.0 = Orthogonal                     â”‚              â”‚
â”‚     â”‚   -1.0 = Opposite direction             â”‚              â”‚
â”‚     â”‚                                          â”‚              â”‚
â”‚     â”‚  Our Result: 0.861 (86.1%)              â”‚              â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                                               â”‚
â”‚  2. TOP-K RETRIEVAL ACCURACY                                  â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚     â”‚  Given batch of B predictions & targets  â”‚              â”‚
â”‚     â”‚                                          â”‚              â”‚
â”‚     â”‚  Similarity Matrix S[i,j]:               â”‚              â”‚
â”‚     â”‚    S[i,j] = pred[i] Â· target[j]         â”‚              â”‚
â”‚     â”‚                                          â”‚              â”‚
â”‚     â”‚  Top-1: Does pred[i] match target[i]?   â”‚              â”‚
â”‚     â”‚    accuracy = Î£(argmax_j S[i,j] == i)/B â”‚              â”‚
â”‚     â”‚                                          â”‚              â”‚
â”‚     â”‚  Top-5: Is target[i] in top-5 for pred[i]? â”‚           â”‚
â”‚     â”‚                                          â”‚              â”‚
â”‚     â”‚  Our Results:                            â”‚              â”‚
â”‚     â”‚    Top-1: 74.54%                        â”‚              â”‚
â”‚     â”‚    Top-5: 99.38%                        â”‚              â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                                               â”‚
â”‚  3. EMBEDDING VARIANCE (Collapse Detection)                   â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚     â”‚  var = mean(var_per_dimension)          â”‚              â”‚
â”‚     â”‚                                          â”‚              â”‚
â”‚     â”‚  High variance (>0.01): Good diversity   â”‚              â”‚
â”‚     â”‚  Low variance (<0.001): Collapse!       â”‚              â”‚
â”‚     â”‚                                          â”‚              â”‚
â”‚     â”‚  Our Result: ~0.0026 (healthy)          â”‚              â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                                                               â”‚
â”‚  4. SILHOUETTE SCORE (Cluster Quality)                        â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚     â”‚  s = (b - a) / max(a, b)                â”‚              â”‚
â”‚     â”‚                                          â”‚              â”‚
â”‚     â”‚  a = avg distance to same cluster        â”‚              â”‚
â”‚     â”‚  b = avg distance to nearest cluster     â”‚              â”‚
â”‚     â”‚                                          â”‚              â”‚
â”‚     â”‚  Range: [-1, 1]                         â”‚              â”‚
â”‚     â”‚    >0.5: Well-separated clusters         â”‚              â”‚
â”‚     â”‚    0.2-0.5: Moderate structure           â”‚              â”‚
â”‚     â”‚    <0.2: Weak/overlapping                â”‚              â”‚
â”‚     â”‚                                          â”‚              â”‚
â”‚     â”‚  Our Result: 0.227 (moderate, expected)  â”‚              â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Visualization Gallery

```
GENERATED VISUALIZATIONS:

1. embeddings_pca.png
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚        PCA Projection (2D)          â”‚
   â”‚                                     â”‚
   â”‚     â€¢  â€¢    â€¢  â€¢ â€¢                  â”‚
   â”‚   â€¢  â€¢ â€¢  â€¢  â€¢   â€¢  â€¢               â”‚
   â”‚  â€¢    â€¢   â€¢      â€¢    â€¢             â”‚
   â”‚     â€¢   â€¢   â€¢  â€¢    â€¢               â”‚
   â”‚   â€¢       â€¢  â€¢   â€¢                  â”‚
   â”‚                                     â”‚
   â”‚  Blue = Predictions                 â”‚
   â”‚  Red = Targets                      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

2. embeddings_clustered.png
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚      K-Means Clusters (k=5)         â”‚
   â”‚                                     â”‚
   â”‚   Cluster 0: â–ˆ â–ˆ â–ˆ                  â”‚
   â”‚   Cluster 1: â–“ â–“ â–“                  â”‚
   â”‚   Cluster 2: â–’ â–’ â–’                  â”‚
   â”‚   Cluster 3: â–‘ â–‘ â–‘                  â”‚
   â”‚   Cluster 4: â–ª â–ª â–ª                  â”‚
   â”‚                                     â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

3. similarity_distributions.png
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚    Pred-Target Similarity Dist      â”‚
   â”‚         ___                         â”‚
   â”‚        /   \                        â”‚
   â”‚       /     \                       â”‚
   â”‚      /       \___                   â”‚
   â”‚     /            \                  â”‚
   â”‚  __/              \_____            â”‚
   â”‚ -1.0    0.0    0.5    1.0          â”‚
   â”‚                                     â”‚
   â”‚  Mean: 0.861 (strong alignment)     â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

4. heatmap_predictions.png
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Embedding Heatmap (200 samples)    â”‚
   â”‚                                     â”‚
   â”‚  Sample  â–“â–’â–‘â–“â–’â–‘â–“â–’â–‘â–“â–’â–‘  384 dims    â”‚
   â”‚    1     â–“â–’â–‘â–“â–’â–‘â–“â–’â–‘â–“â–’â–‘               â”‚
   â”‚    2     â–’â–‘â–“â–’â–‘â–“â–’â–‘â–“â–’â–‘â–“               â”‚
   â”‚   ...    â–‘â–“â–’â–‘â–“â–’â–‘â–“â–’â–‘â–“â–’               â”‚
   â”‚   200    â–“â–’â–‘â–“â–’â–‘â–“â–’â–‘â–“â–’â–‘               â”‚
   â”‚                                     â”‚
   â”‚  Rich activation patterns across    â”‚
   â”‚  all dimensions (no collapse)       â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸš€ Usage Examples

### 1. Train from Scratch

```bash
# Generate dataset
python graph_jepa_dataset_creation.py \
    --nodes nodes.jsonl \
    --edges edges.jsonl \
    --out dataset.jsonl \
    --n_snapshots 200 \
    --anchors_per_snapshot 64

# Train model
python train_jepa_v0.py \
    --dataset dataset.jsonl \
    --out_dir checkpoints_v0_b16 \
    --steps 40000 \
    --batch_size 16 \
    --lr 1e-4 \
    --device mps \
    --tb_dir runs/jepa_v0_b16
```

### 2. Resume Training

```bash
python train_jepa_v0.py \
    --dataset dataset.jsonl \
    --resume checkpoints_v0_b16/ckpt_step_20000.pt \
    --steps 40000 \
    --device mps
```

### 3. Run Inference

```bash
python inference.py \
    --checkpoint checkpoints_v0_b16/ckpt_final.pt \
    --dataset dataset.jsonl \
    --output_dir inference_results \
    --n_clusters 5 \
    --device mps
```

### 4. Visualize Embeddings

```bash
python visualize_embeddings.py
# Generates detailed visualizations in inference_results/detailed_viz/
```

### 5. Monitor Training

```bash
tensorboard --logdir runs/jepa_v0_b16 --port 6006
# Open http://localhost:6006 in browser
```

---

## ðŸŽ¯ Key Takeaways

### What Makes This Architecture Special?

1. **Self-Supervised Learning**
   - No manual labels needed
   - Learns from graph structure itself
   - Scalable to large datasets

2. **Temporal Awareness**
   - Time features integrated into nodes
   - Validity windows respected
   - Can model graph evolution

3. **Relation-Aware**
   - Thousands of edge types handled efficiently
   - Relation embeddings learned end-to-end
   - Captures semantic relationships

4. **Robust to Corruption**
   - Trained on masked, incomplete graphs
   - Learns to infer from partial information
   - Generalizes to noisy real-world data

5. **Student-Teacher Stability**
   - EMA prevents collapse
   - Stable training dynamics
   - No need for negative sampling

### When to Use This Model?

âœ… **Good for:**
- Knowledge graph embedding
- Graph similarity search
- Link prediction
- Temporal forecasting
- Transfer learning (pre-training)

âŒ **Not ideal for:**
- Node-level classification (without fine-tuning)
- Small graphs (<10 nodes)
- Static graphs (doesn't leverage temporal info)
- Real-time inference (Transformer is slow)

---

## ðŸ“š Further Reading

**For Implementation Details:**
- See `RESEARCH_PAPER.md` for full technical paper
- See `EXECUTIVE_SUMMARY.md` for quick overview
- See `dataset_analysis.md` for data structure

**For Code:**
- `train_jepa_v0.py`: Training loop and model definition
- `inference.py`: Evaluation and analysis
- `graph_jepa_dataset_creation.py`: Dataset generation

**For Results:**
- `inference_results/`: Evaluation outputs
- `runs/`: TensorBoard logs
- `checkpoints_v0_b16/`: Trained models

---

*This architecture guide provides a visual, diagram-rich explanation of the Graph JEPA model. For mathematical details and experimental results, see the full research paper.*

**Last Updated:** February 2026

